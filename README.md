# GlenX
Tool for better genotyping of whole genome sequencing (WGS) data. 

![alt text](https://github.com/vborjesson/GlenX/blob/master/Glen.png)

### Startup project: 
Try to find a good de novo assembly tool. I try Velvet, abyss and spades. To align the assembled reads (scaffolds) back to the ref genome again, I use bwa mem. 

#### What I tried so far: 

Get reads from specific region (start +/- 1kb - end +/- 1kb): 

```
samtools view -b file.bam "chrX:XXXXXX-XXXXX" "chrX:XXXXXX-XXXXXX" > regionX.bam
samtools view regionX.bam | awk '{print ">"$1"\n"$10}' > regionX.fasta

velveth velveth_out regionX.fasta
abyss-pe k-50 name=name in=regionX.fasta
spades.py --12 regionX.bam -o spades_out 

bwa mem -x intractg ref.fa assembled.fasta > aligned_assambled.sam
```
Modules needed in order to run GlenX; 
	samtools
	ABYSS
	bwa

Genotyping and classify type of SV;
From the mapped contigs a sam-file is generated. sam-file gives us information about direction of the reads and if they are matched or not matched to reference genome. 
Looking at the NOT matched contigs (S), I will get information about a SV and where it starts and ends. If this is mapped inside our regions of interest, we will call it a SV. And if we also have a contig that maps as match (M) to the reference genome, we can classify it as a heterozygous SV.   

Thoughts; 
go back to fasta-file generated by assembly tool, information about statistics? 

2017 - 09 - 26
	A few changes most be done on GlenX - genotyping. 
		1.	done	Read in TIDDIT tab-file with read coverage for every 100th bp.
		2.	done	Count average coverage 200 bp (exclude x- and y-chromosome due to different coverage?) 
		3. 	done	Update counting breakpoints adding D as important information. Take into count reverse strand (flip cigar). 
		4. 	done 	Make new array saving soft clips; strand, chr, pos, score, strand, chr, pos, score, cigar, contig length, number of alternative pos)					Have also added average position coverage.
		5. 	done	Save match as before in array! 
		6. 	done	Compare the two arrays, if not in region discard! 
		7. 	skip!	Get assembly read coverage from fasta, number of absolute kmers(?) that match to contig.
		8. 	skip! 	better scoring?		If more than 1 in array; make some kind of scoring, selecting the most important one. For ex. a lot of cigars (complex), very short contig, a lot of alternative positions, low score etc.. Best score stays!
		9. 	done	lÃ¤gg till SAKE (ev andra tools?) i asssembly.sh
		9 			Look at CDHIT for clustering instead. 
		10. 		Add gc content and mappability to new array 100 bp
		11. 		Look at old tab files of coverage. mappability conserved regions (low chance of dup and dels) with low and high mappability. how does it look? Should be the same for whole region. 
		10. 		Classify using tab-array, strand, position etc. Counting standard deviation and mean of entire chromosome for comparison with break-point region. NOTE! This will not work since the data are not normally distributed. First i have to normalize the data. Ex using gc normalization, mappability etc. Make script for normalization. Mappability download file. BND, INV, DUP, DEL, TDUP (tandem dup) 
		11. 		Make plot and more plots. What about gc-content vs read coverage => can we find any correlations? 
					Mappability? 
		12. 			Write to new vcf-file! 

	
	GlenX - assembly
		1.	done	continue writing assambly.sh script. Run abyss * 3 (30, 50 and 70)
		2. 	done	Change id. for an example; >1 will be changed to >kmer_30_1
		3. 	done	merge 3 fasta-files to 1. 
		4.	done	Put bwa mem into assambly.sh
		5. 			Test!


Next time, continue:
-> Currently running on sbatch. Make simulated data for benchmarking
-> 		Look at Velveth again, is it good enough to use in pipeline? Cant find any mating mapping position.
-> 		is clustering really the best idea to find the best predicted breakpoints? If there is a really short contig and a long one?   
-> 		Run SVenX on Bianca!
ongoing, problems with installing nextflow
-> 		Take care of biases. Normalize data. GC-bias and  mappability. make script to do a table (array 2*many) with the data. Run before GlenX. 


Meeting 4th of October:
Installing Anaconda in order for svdb to work.
Transfer simulated 10x-data to Bianca. 
Transfer SVDB to SVenX Bianca.
Start SVenX * 2. One run with real patient data and one with simulated data.
Calculate GC-content /100bp
Make SQLite with chromosome, position 100bp, GC-content and mappability. 
